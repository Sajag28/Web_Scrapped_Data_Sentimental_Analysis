{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_P-Mq0_WF4lsysT1vEG9V6gxxL_9kBMF",
      "authorship_tag": "ABX9TyOYG9UBcIvzmTDWTD5r03Nk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sajag28/Web_Scrapped_Data_Sentimental_Analysis/blob/main/Web_Scrapped_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKGZbJn828_w",
        "outputId": "0e9f3395-7394-4882-c4de-ca33ad3314b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path ='/content/drive/MyDrive/Black_Trial/Input.xlsx'\n",
        "import openpyxl\n",
        "links=[]\n",
        "workbook = openpyxl.load_workbook(file_path)\n",
        "worksheet = workbook['Sheet1']\n",
        "for i in range(2,101):\n",
        " cell_value = worksheet[f'B{i}'].value\n",
        " links.append(cell_value)\n",
        "for i in range(0,5):\n",
        "  print(links[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqybbaHF5oN4",
        "outputId": "e9c4c4f7-318e-4dc6-a146-e2d5f67f7d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/\n",
            "https://insights.blackcoffer.com/rising-it-cities-and-their-impact-on-the-economy-environment-infrastructure-and-city-life-in-future/\n",
            "https://insights.blackcoffer.com/internet-demands-evolution-communication-impact-and-2035s-alternative-pathways/\n",
            "https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-in-upcoming-future/\n",
            "https://insights.blackcoffer.com/ott-platform-and-its-impact-on-the-entertainment-industry-in-future/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNsXvv0N-fTp",
        "outputId": "20cb7b94-aec8-4ca9-d662-bcbf1a34cb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('cmudict')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "231BOpJVY_y4",
        "outputId": "4f692643-ff1f-47ce-8d69-c493596e7784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xetze6Y5gjyY",
        "outputId": "eb12e976-e1c2-4080-e584-c04c68751342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from string import punctuation\n",
        "import nltk\n",
        "import re\n",
        "import requests\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def calculate_readability_metrics(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "    avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)\n",
        "    num_complex_words = sum(len(w) > 2 for w in words)\n",
        "    fog_index = 0.4 * (avg_sentence_length + 100 * (num_complex_words / len(words)))\n",
        "    avg_words_per_sentence = len(words) / len(sentences)\n",
        "    # syllable_per_word = sum(nltk.corpus.cmudict.dict.get(w.lower(), ())[0][-1] for w in words if w.lower() in nltk.corpus.cmudict.dict) / len(words)\n",
        "    personal_pronouns = sum(w.lower() in {'i', 'we', 'you', 'he', 'she', 'it', 'they', 'me', 'us', 'him', 'her', 'them'} for w in words)\n",
        "    avg_word_length = sum(len(w) for w in words) / len(words)\n",
        "\n",
        "    return {\n",
        "        \"avg_sentence_length\": avg_sentence_length,\n",
        "        \"percentage_complex_words\": num_complex_words / len(words) * 100,\n",
        "        \"fog_index\": fog_index,\n",
        "        \"avg_words_per_sentence\": avg_words_per_sentence,\n",
        "        \"complex_word_count\": num_complex_words,\n",
        "        \"word_count\": len(words),\n",
        "        # \"syllable_per_word\": syllable_per_word,\n",
        "        \"personal_pronouns\": personal_pronouns,\n",
        "        \"avg_word_length\": avg_word_length,\n",
        "    }\n",
        "\n",
        "results=[]\n",
        "for i in range(2,87):\n",
        "    url = links[i]\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        title_text = soup.title.text.strip()\n",
        "        div_with_class = soup.find('div', class_=\"td-post-content tagdiv-type\")\n",
        "\n",
        "        if div_with_class:\n",
        "            div_paragraphs = div_with_class.find_all(['p','strong','li'])\n",
        "            div_text = '\\n\\n'.join(p.get_text(strip=True) for p in div_paragraphs)\n",
        "            clean_text = title_text + '\\n' + div_text\n",
        "\n",
        "\n",
        "            stop_words = []\n",
        "            for filename in [\n",
        "                \"StopWords_Auditor.txt\",\n",
        "                \"StopWords_Currencies.txt\",\n",
        "                \"StopWords_DatesandNumbers.txt\",\n",
        "                \"StopWords_GenericLong.txt\",\n",
        "                \"StopWords_Generic.txt\",\n",
        "                \"StopWords_Geographic.txt\",\n",
        "                \"StopWords_Names.txt\",\n",
        "            ]:\n",
        "                try:\n",
        "                    with open(\"/content/drive/MyDrive/Black_Trial/\" + filename, \"r\", encoding=\"utf-8\") as file:\n",
        "                        stop_words.extend(file.read().splitlines())\n",
        "                except UnicodeDecodeError:\n",
        "                    # print(f\"Warning: Unable to decode {filename} with UTF-8 encoding.\")\n",
        "                    print(\" \")\n",
        "\n",
        "            pattern = re.compile('|'.join(re.escape(word) for word in stop_words))\n",
        "            filtered_content = pattern.sub(' ', clean_text)\n",
        "\n",
        "\n",
        "            # print(clean_text)\n",
        "            words_to_include = set()\n",
        "            for filename in [\"negative-words.txt\", \"positive-words.txt\"]:\n",
        "              with open(\"/content/drive/MyDrive/Black_Trial/MasterDictionary/\" + filename, \"r\", encoding=\"utf-8\") as file:\n",
        "                try:\n",
        "                 with open(\"/content/drive/MyDrive/Black_Trial/MasterDictionary/\" + filename, \"r\", encoding=\"utf-8\") as file:\n",
        "                    words_to_include.update(file.read().splitlines())\n",
        "                except UnicodeDecodeError:\n",
        "                  #  print(f\"Warning: Unable to decode {filename} with UTF-8 encoding. Trying a different encoding.\")\n",
        "                   try:\n",
        "                      with open(\"/content/drive/MyDrive/Black_Trial/MasterDictionary/\" + filename, \"r\", encoding=\"latin-1\") as file:\n",
        "                        words_to_include.update(file.read().splitlines())\n",
        "                   except UnicodeDecodeError:\n",
        "                    #  print(f\"Error: Unable to decode {filename} with either UTF-8 or latin-1 encoding.\")\n",
        "                    print(\" \")\n",
        "                for word in clean_text.split():\n",
        "                  if word in words_to_include:\n",
        "                   filtered_content=filtered_content+word\n",
        "\n",
        "\n",
        "            final_filtered_content = ' '.join(filtered_content)\n",
        "\n",
        "            # print(f\"Filtered content (including specified words): {final_filtered_content}\")\n",
        "            from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "            analyzer = SentimentIntensityAnalyzer()\n",
        "            scores = analyzer.polarity_scores(final_filtered_content)\n",
        "            readability_metrics = calculate_readability_metrics(final_filtered_content)\n",
        "\n",
        "            results.append({\n",
        "                \"URL\": url,\n",
        "                \"POSITIVESCORE\":scores['pos'],\n",
        "                \"NEGATIVESCORE\":scores['neg'],\n",
        "                \"POLARITYSCORE\": scores['compound'],  # Add polarity score\n",
        "                \"SUBJECTIVITYSCORE\": scores['neu'],\n",
        "                \"AVERAGE SENTENCE LENGTH\": readability_metrics['avg_sentence_length'],\n",
        "                \"PERCENTAGE OF COMPLEX WORDS\":readability_metrics['percentage_complex_words'],\n",
        "                \"FOGINDEX\":readability_metrics['fog_index'],\n",
        "                \"AVG NUMBER OF WORDS\":readability_metrics['avg_words_per_sentence'],\n",
        "                \"COMPLEX WORD COUNT\":readability_metrics['complex_word_count'],\n",
        "                \"PERSONAL PRONOUNS\":readability_metrics['personal_pronouns'],\n",
        "                \"AVG WORD LENGTH\":readability_metrics['avg_word_length']\n",
        "            })\n",
        "            # print(scores)\n",
        "            # print(results)\n",
        "            df = pd.DataFrame(results)\n",
        "            df.to_excel('sentiment_data_final_2.xlsx', index=False)\n",
        "\n",
        "        else:\n",
        "            print(\"Div with class 'td-post-content tagdiv-type' not found on the page.\")\n",
        "    else:\n",
        "        print(\"Error: Unable to fetch the page.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AzJ3GZpZB2ZJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}